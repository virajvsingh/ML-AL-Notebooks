{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b18e353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Importing\n",
      "import finished\n"
     ]
    }
   ],
   "source": [
    "# importing all the libraries needed\n",
    "print(\"Started Importing\")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "print(\"import finished\")  #have added a lot of print commnad cuz i needed updates and sometimes it took mintues to get results\n",
    "#tried XGB Boost but didn't find better results. It started to reduce the score actually. \n",
    "#Maybe cuz I gave very less time to work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fdcd3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# loading all the files given to work with them using pandas\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(\"train.csv\")\n",
    "    train_labels_df = pd.read_csv(\"train_labels.csv\")\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    print(\"Data loaded successfully.\")\n",
    "    \n",
    "except FileNotFoundError as error_shown:\n",
    "    # making sure the program doesn't end with an error code\n",
    "    print(f\"Error loading file: {error_shown}.Ensure the files are in the same directory.\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "187f1f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Dropped 250 rows from training data due to missing 'Class' labels.\n",
      "Training features shape: (150, 14572)\n",
      "Training target shape: (150,)\n",
      "Test data shape: (401, 14572)\n"
     ]
    }
   ],
   "source": [
    "# working with the dataframes created above\n",
    "\n",
    "#checking if drain_df containes 'Class' to remove that coulmn from the dataframe\n",
    "for i in train_df.columns:\n",
    "    if i == 'Class':    # to check if one the column name is Class\n",
    "        train_df = train_df.drop(columns=['Class'])\n",
    "        \n",
    "# merging training data with the labels(gene data and cancer classes) present in another file\n",
    "# using 'Id' as it is present in both cases\n",
    "\n",
    "train_merged_df = pd.merge(train_df, train_labels_df, on='Id', how='left')\n",
    "\n",
    "# Checking for NaNs in the 'Class' column after merge as there were some NaNs values causing errors in the results.\n",
    "initial_rows = len(train_merged_df)  # counting the number of initial tuples available in the training data\n",
    "\n",
    "# removed all the rows from the training data that had a NaN value and checked the number of rows in dataframe\n",
    "train_merged_df.dropna(subset=['Class'], inplace=True) \n",
    "rows_after_drop = len(train_merged_df)\n",
    "\n",
    "if initial_rows > rows_after_drop:\n",
    "    print(f\"Warning: Dropped {initial_rows - rows_after_drop} rows from training data due to missing 'Class' labels.\")\n",
    "else:\n",
    "    print(\"No missing 'Class' labels found in training data after merge.\")\n",
    "    \n",
    "#we create an empty list that stores the names of columns and then loop through df\n",
    "#then checking if it begins with 'gene' and if it does it is appended to the list\n",
    "#using these to form X_train and Y_train\n",
    "\n",
    "columns_list = []\n",
    "for columns in train_merged_df.columns:\n",
    "    if columns.startswith('gene_'):\n",
    "        columns_list.append(columns)\n",
    "\n",
    "X_train = train_merged_df[columns_list]\n",
    "y_train = train_merged_df['Class']\n",
    "\n",
    "if test_df.empty:\n",
    "    print(\"Error: test.csv is empty. Cannot create X_test.\")\n",
    "    exit()\n",
    "X_test = test_df[columns_list] \n",
    "\n",
    "#printing info\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Training target shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# creating a preprocessing and modeling pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')), # Impute missing values with the mean\n",
    "    ('scaler', StandardScaler()),                 # Scale features to have zero mean and unit variance\n",
    "    ('classifier', RandomForestClassifier(n_estimators=200, # Initially it was 100 but got better results with 200\n",
    "                                         class_weight='balanced', # Added to handle class imbalance\n",
    "                                         random_state=42,\n",
    "                                         n_jobs=-1))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb311f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "print(\"Training model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a54d5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test data...\n",
      "Predictions generated.\n",
      "Creating submission file...\n",
      "Submission file 'submission.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "#creating predictions and generating submission data\n",
    "print(\"Generating predictions on test data...\")\n",
    "predictions = pipeline.predict(X_test)\n",
    "print(\"Predictions generated.\")\n",
    "\n",
    "print(\"Creating submission file...\")\n",
    "submission_df = pd.DataFrame({'Id': test_df['Id'], 'Class': predictions})\n",
    "\n",
    "# making sure that the datatype of 'Class' is integer\n",
    "submission_df['Class'] = submission_df['Class'].astype(int)\n",
    "\n",
    "# Save the submission file\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "print(f\"Submission file '{submission_file_name}' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
