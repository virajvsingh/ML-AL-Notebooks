{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bc8ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing\n",
      "Preprocessing complete.\n",
      "Training Logistic Regression model...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best parameters found: {'C': 100, 'penalty': 'l1'}\n",
      "Best cross-validation accuracy: 0.8454\n",
      "Making predictions on the test set\n",
      "'submission.csv' created successfully!\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.fft import fft\n",
    "from scipy.integrate import trapz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Loading the datasets\n",
    "try:\n",
    "    train_df = pd.read_csv('C:\\\\Users\\\\viraj\\\\Downloads\\\\hacktrain.csv')\n",
    "    test_df = pd.read_csv('C:\\\\Users\\\\viraj\\\\Downloads\\\\hacktest.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: file not found.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "ndvi_cols = [col for col in train_df.columns if '_N' in col]\n",
    "\n",
    "def preprocess_ndvi_data(df, ndvi_cols):\n",
    "    for col in ndvi_cols:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "    df[ndvi_cols] = df[ndvi_cols].interpolate(method='linear', axis=1, limit_direction='both')\n",
    "    df[ndvi_cols] = df[ndvi_cols].fillna(0)\n",
    "\n",
    "    window_length = 7 \n",
    "    polyorder = 2\n",
    "    \n",
    "    denoised_ndvi_values = []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            denoised_ndvi = savgol_filter(row[ndvi_cols].values, window_length, polyorder)\n",
    "            denoised_ndvi_values.append(denoised_ndvi)\n",
    "        except ValueError:\n",
    "            denoised_ndvi_values.append(row[ndvi_cols].values)\n",
    "    \n",
    "    df[ndvi_cols] = pd.DataFrame(denoised_ndvi_values, index=df.index, columns=ndvi_cols)\n",
    "    return df\n",
    "\n",
    "print(\"Applying preprocessing\")\n",
    "train_df_processed = preprocess_ndvi_data(train_df.copy(), ndvi_cols)\n",
    "test_df_processed = preprocess_ndvi_data(test_df.copy(), ndvi_cols)\n",
    "print(\"Preprocessing complete.\")\n",
    "\n",
    "def create_enhanced_features(df, ndvi_cols):\n",
    "    features_df = pd.DataFrame(df['ID'])\n",
    "\n",
    "    features_df['ndvi_mean'] = df[ndvi_cols].mean(axis=1)\n",
    "    features_df['ndvi_median'] = df[ndvi_cols].median(axis=1)\n",
    "    features_df['ndvi_min'] = df[ndvi_cols].min(axis=1)\n",
    "    features_df['ndvi_max'] = df[ndvi_cols].max(axis=1)\n",
    "    features_df['ndvi_std'] = df[ndvi_cols].std(axis=1)\n",
    "    features_df['ndvi_range'] = features_df['ndvi_max'] - features_df['ndvi_min']\n",
    "    \n",
    "\n",
    "    features_df['ndvi_auc'] = df[ndvi_cols].apply(lambda x: trapz(x), axis=1)\n",
    "\n",
    "    def get_phenology_features(row_series):\n",
    "        peak_idx = row_series.argmax()\n",
    "        trough_idx = row_series.argmin()\n",
    "        \n",
    "        peak_timing = peak_idx / len(ndvi_cols)\n",
    "        trough_timing = trough_idx / len(ndvi_cols)\n",
    "        \n",
    "        peak_val = row_series.iloc[peak_idx]\n",
    "        trough_val = row_series.iloc[trough_idx]\n",
    "        \n",
    "        if peak_timing > trough_timing:\n",
    "            growth_slope = (peak_val - trough_val) / (peak_timing - trough_timing)\n",
    "        else:\n",
    "            growth_slope = 0\n",
    "            \n",
    "        if peak_timing < 1.0: \n",
    "             \n",
    "             end_val = row_series.iloc[-1]\n",
    "             senescence_slope = (peak_val - end_val) / (1.0 - peak_timing)\n",
    "        else:\n",
    "             senescence_slope = 0 \n",
    "\n",
    "        return peak_timing, trough_timing, growth_slope, senescence_slope\n",
    "\n",
    "    phenology_features = df[ndvi_cols].apply(get_phenology_features, axis=1, result_type='expand')\n",
    "    phenology_features.columns = ['peak_timing', 'trough_timing', 'growth_slope', 'senescence_slope']\n",
    "    features_df = pd.concat([features_df, phenology_features], axis=1)\n",
    "    \n",
    "   \n",
    "    for index, row in df.iterrows():\n",
    "        fft_result = fft(row[ndvi_cols].values)\n",
    "        num_components = 5 \n",
    "        magnitudes = np.abs(fft_result[1:num_components+1])\n",
    "        for i in range(num_components):\n",
    "            features_df.loc[index, f'fft_mag_{i}'] = magnitudes[i]\n",
    "\n",
    "    \n",
    "    features_df['mean_x_std'] = features_df['ndvi_mean'] * features_df['ndvi_std']\n",
    "    features_df['range_x_peak_timing'] = features_df['ndvi_range'] * features_df['peak_timing']\n",
    "    \n",
    "    features_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    features_df.fillna(0, inplace=True)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "X_train_features = create_enhanced_features(train_df_processed, ndvi_cols)\n",
    "X_test_features = create_enhanced_features(test_df_processed, ndvi_cols)\n",
    "\n",
    "common_cols = list(set(X_train_features.columns) & set(X_test_features.columns))\n",
    "feature_cols_to_use = [col for col in common_cols if col != 'ID']\n",
    "\n",
    "X_train_final = X_train_features[feature_cols_to_use]\n",
    "X_test_final = X_test_features[feature_cols_to_use]\n",
    "y_train = train_df_processed['class']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "X_test_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_cols_to_use, index=X_train_final.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_cols_to_use, index=X_test_final.index)\n",
    "\n",
    "\n",
    "print(\"Training Logistic Regression model...\")\n",
    "\n",
    "logistic_model = LogisticRegression(multi_class='multinomial', solver='saga', max_iter=5000, random_state=42)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(logistic_model, param_grid, cv=skf, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_scaled_df, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "print(\"Making predictions on the test set\")\n",
    "predictions = best_model.predict(X_test_scaled_df)\n",
    "\n",
    "submission_df = pd.DataFrame({'ID': test_df['ID'], 'class': predictions})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"'submission.csv' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97d3606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
